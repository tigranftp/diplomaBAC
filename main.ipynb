{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "import pickle\n",
    "from time import time\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense, Flatten, Reshape, concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers.merge import add\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras import Input, layers\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load descriptions\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def load_descriptions(doc):  \n",
    "    mapping = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        image_id = image_id.split('.')[0]\n",
    "        image_desc = ' '.join(image_desc)\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = list()\n",
    "        mapping[image_id].append(image_desc)\n",
    "    return mapping\n",
    "  \n",
    "def clean_descriptions(descriptions):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            desc = desc_list[i]\n",
    "            desc = desc.split()\n",
    "            desc = [word.lower() for word in desc]\n",
    "            desc = [w.translate(table) for w in desc]\n",
    "            desc = [word for word in desc if len(word)>1]\n",
    "            desc = [word for word in desc if word.isalpha()]\n",
    "            desc =  ' '.join(desc)\n",
    "            desc = desc.replace(\"merrygoround\", \"apples\")\n",
    "            desc = desc.replace(\"tshirt\", \"t-shirt\")\n",
    "            desc = desc.replace(\"sweat-shirt\", \"sweatshirt\")\n",
    "            desc = desc.replace(\"upsidedown\", \"upside-down\")\n",
    "            desc = desc.replace(\"snowcovered\", \"snow-covered\")\n",
    "            desc = desc.replace(\"rollerblader\", \"person\")\n",
    "            desc = desc.replace(\"waterskier\", \"person\")\n",
    "            desc = desc.replace(\"floaties\", \"floaty\")\n",
    "            desc_list[i] = desc\n",
    "    return descriptions\n",
    "\n",
    "# save descriptions to file, one per line\n",
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + ' ' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        if image_id in dataset:\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions\n",
    "  \n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    for line in doc.split('\\n'):\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)\n",
    "\n",
    "# load training dataset \n",
    "\n",
    "\n",
    "filename = \"dataset/Flickr8k_text/Flickr8k.token.txt\"\n",
    "doc = load_doc(filename)\n",
    "descriptions = load_descriptions(doc)\n",
    "descriptions = clean_descriptions(descriptions)\n",
    "save_descriptions(descriptions, 'descriptions.txt')\n",
    "filename = 'dataset/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "filename = 'dataset/Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "val = load_set(filename)\n",
    "val_descriptions = load_clean_descriptions('descriptions.txt', val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed words 7575 -> 1649\n"
     ]
    }
   ],
   "source": [
    "# Create a list of all the training captions\n",
    "all_train_captions = []\n",
    "for key, val in train_descriptions.items():\n",
    "    for cap in val:\n",
    "        all_train_captions.append(cap)\n",
    "        \n",
    "        \n",
    "# Consider only words which occur at least 10 times in the corpus\n",
    "word_count_threshold = 10\n",
    "word_counts = {}\n",
    "nsents = 0\n",
    "for sent in all_train_captions:\n",
    "    nsents += 1\n",
    "    for w in sent.split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "print('Preprocessed words {} -> {}'.format(len(word_counts), len(vocab)))\n",
    "\n",
    "\n",
    "ixtoword = {}\n",
    "wordtoix = {}\n",
    "\n",
    "ix = 1\n",
    "for w in vocab:\n",
    "    wordtoix[w] = ix\n",
    "    ixtoword[ix] = w\n",
    "    ix += 1\n",
    "    \n",
    "vocab_size = len(ixtoword) + 1 # one for appended 0's\n",
    "\n",
    "# Load Glove vectors\n",
    "glove_dir = 'glove.6B'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "embedding_dim = 200\n",
    "\n",
    "# Get 200-dim dense vector for each of the words in out vocabulary\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in wordtoix.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startseq 30000\n",
      "endseq 30000\n"
     ]
    }
   ],
   "source": [
    "#список слов, которых нет в GloVe\n",
    "for word, _ in wordtoix.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is None:\n",
    "        print(word, word_counts[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images_path = 'dataset/Flicker8k_Dataset/'\n",
    "# Create a list of all image names in the directory\n",
    "all_images = glob.glob(all_images_path + '*.jpg')\n",
    "\n",
    "# Create a list of all the training and testing images with their full path names\n",
    "def create_list_of_images(file_path):\n",
    "    images_names = set(open(file_path, 'r').read().strip().split('\\n'))\n",
    "    images = []\n",
    "\n",
    "    for image in all_images: \n",
    "        if image[len(all_images_path):] in images_names:\n",
    "            images.append(image)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "train_images_path = 'dataset/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "test_images_path = 'dataset/Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "val_images_path = 'dataset/Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "\n",
    "train_images = create_list_of_images(train_images_path)\n",
    "test_images = create_list_of_images(test_images_path)\n",
    "val_images = create_list_of_images(val_images_path)\n",
    "\n",
    "#preprocessing the images\n",
    "def preprocess(image_path):\n",
    "    img = image.load_img(image_path, target_size=(299, 299))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "# Load the inception v3 model\n",
    "model = InceptionV3(weights='imagenet')\n",
    "\n",
    "# Create a new model, by removing the last layer (output layer) from the inception v3\n",
    "model_new = Model(model.input, model.layers[-2].output)\n",
    "\n",
    "# Encoding a given image into a vector of size (2048, )\n",
    "def encode(image):\n",
    "    image = preprocess(image) \n",
    "    fea_vec = model_new.predict(image) \n",
    "    fea_vec = np.reshape(fea_vec, fea_vec.shape[1])\n",
    "    return fea_vec\n",
    "\n",
    "encoding_train = {}\n",
    "for img in train_images:\n",
    "    encoding_train[img[len(all_images_path):]] = encode(img)\n",
    "\n",
    "\n",
    "encoding_test = {}\n",
    "for img in test_images:\n",
    "    encoding_test[img[len(all_images_path):]] = encode(img)\n",
    "\n",
    "encoding_val = {}\n",
    "for img in val_images:\n",
    "    encoding_val[img[len(all_images_path):]] = encode(img)\n",
    "\n",
    "# Save the bottleneck features to disk\n",
    "with open(\"encoded_files/encoded_train_images.pkl\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(encoding_train, encoded_pickle)\n",
    "\n",
    "with open(\"encoded_files/encoded_test_images.pkl\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(encoding_test, encoded_pickle)\n",
    "\n",
    "\n",
    "with open(\"encoded_files/encoded_val_images.pkl\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(encoding_val, encoded_pickle)    \n",
    "\n",
    "train_features = pickle.load(open(\"encoded_files/encoded_train_images.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n=0\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            n+=1\n",
    "            # retrieve the photo feature\n",
    "            photo = photos[key+'.jpg']\n",
    "            for desc in desc_list:\n",
    "                # encode the sequence\n",
    "                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
    "                # split one sequence into multiple X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # split into input and output pair\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # encode output sequence\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    # store\n",
    "                    X1.append(photo)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            # yield the batch data\n",
    "            if n==num_photos_per_batch:\n",
    "                yield [[array(X1), array(X2)], array(y)]\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2000/2000 [==============================] - 268s 133ms/step - loss: 4.1176\n",
      "Epoch 2/20\n",
      "2000/2000 [==============================] - 260s 130ms/step - loss: 3.4170\n",
      "Epoch 3/20\n",
      "2000/2000 [==============================] - 257s 129ms/step - loss: 3.2009\n",
      "Epoch 4/20\n",
      "2000/2000 [==============================] - 256s 128ms/step - loss: 3.0669\n",
      "Epoch 5/20\n",
      "2000/2000 [==============================] - 278s 139ms/step - loss: 2.9729\n",
      "Epoch 6/20\n",
      "2000/2000 [==============================] - 293s 146ms/step - loss: 2.9012\n",
      "Epoch 7/20\n",
      "2000/2000 [==============================] - 286s 143ms/step - loss: 2.8446\n",
      "Epoch 8/20\n",
      "2000/2000 [==============================] - 260s 130ms/step - loss: 2.7946\n",
      "Epoch 9/20\n",
      "2000/2000 [==============================] - 259s 130ms/step - loss: 2.7523\n",
      "Epoch 10/20\n",
      "2000/2000 [==============================] - 260s 130ms/step - loss: 2.7209\n",
      "Epoch 11/20\n",
      "2000/2000 [==============================] - 257s 129ms/step - loss: 2.6908\n",
      "Epoch 12/20\n",
      "2000/2000 [==============================] - 299s 149ms/step - loss: 2.6647\n",
      "Epoch 13/20\n",
      "2000/2000 [==============================] - 319s 159ms/step - loss: 2.6407\n",
      "Epoch 14/20\n",
      "2000/2000 [==============================] - 267s 134ms/step - loss: 2.6215\n",
      "Epoch 15/20\n",
      "2000/2000 [==============================] - 265s 132ms/step - loss: 2.5994\n",
      "Epoch 16/20\n",
      "2000/2000 [==============================] - 268s 134ms/step - loss: 2.5819\n",
      "Epoch 17/20\n",
      "2000/2000 [==============================] - 267s 133ms/step - loss: 2.5697\n",
      "Epoch 18/20\n",
      "2000/2000 [==============================] - 269s 134ms/step - loss: 2.5548\n",
      "Epoch 19/20\n",
      "2000/2000 [==============================] - 287s 143ms/step - loss: 2.5404\n",
      "Epoch 20/20\n",
      "2000/2000 [==============================] - 302s 151ms/step - loss: 2.5297\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 308s 308ms/step - loss: 2.4981\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 286s 286ms/step - loss: 2.4491\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 2.4282\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 2.4154\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 286s 286ms/step - loss: 2.4032\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 288s 288ms/step - loss: 2.3944\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 288s 288ms/step - loss: 2.3865\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 287s 287ms/step - loss: 2.3820\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 334s 334ms/step - loss: 2.3744\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 365s 365ms/step - loss: 2.3670\n"
     ]
    }
   ],
   "source": [
    "import keras.callbacks\n",
    "max_length1 = 34\n",
    "inputs1 = Input(shape=(2048,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "inputs2 = Input(shape=(max_length1,))\n",
    "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\n",
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "epochs = 20\n",
    "number_pics_per_batch = 3\n",
    "steps = len(train_descriptions)//number_pics_per_batch\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint('checkp_save/model{epoch:08d}.h5', save_freq=2000) \n",
    "generator = data_generator(train_descriptions, train_features, wordtoix, max_length1, number_pics_per_batch)\n",
    "history = model.fit(generator, epochs=20,  steps_per_epoch=steps, verbose=1, callbacks=[checkpoint])\n",
    "\n",
    "\n",
    "model.optimizer.lr = 0.0001\n",
    "epochs = 10\n",
    "number_pics_per_batch = 6\n",
    "steps = len(train_descriptions)//number_pics_per_batch\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint('checkp_save/model2{epoch:08d}.h5', save_freq=1000) \n",
    "\n",
    "generator = data_generator(train_descriptions, train_features, wordtoix, max_length1, number_pics_per_batch)\n",
    "history1 = model.fit(generator, epochs=10, steps_per_epoch=steps, verbose=1, callbacks=[checkpoint])\n",
    "model.save('saved_model/modelswap_' + '34_fit256' + '.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
